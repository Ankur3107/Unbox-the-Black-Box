---
title: "Unbox the Block Box: (Part #1)"
author: "Ankur Singh"
date: "Aug 23, 2018"
output:
 html_document:
    fig_width: 10
    fig_height: 7
    toc: yes
    number_sections : yes
    code_folding: show
---

<center><img src="https://www.tibco.com/blog/wp-content/uploads/2018/06/BeyondBlackBox_image620x360.jpg"></center>


<hr>

<strong>Version Description :</strong>

* _version 1 : initial commit_ 

<hr>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE)
```

# OBJECTIVE

Actually I was thinking about 'how to get the machine learning model explantion'. Then I searched on Google and found a great package called **LIME** by Tulio Ribeiro, Sameer Singh and Carlos Guestrin. I  was very impressed with the package. In this kernel, I will share my knowledge that i got from different papers, blogs and videos.


# LIME : Locally Interpretable Model-agnostic Explanations

<center><iframe width="560" height="315" src="https://www.youtube.com/embed/hUnRCxnydCc" frameborder="0" allowfullscreen></iframe></center>

## ABSTRACT

Machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.

## WHAT IS LIME

* **Locally**: Locally faithful to the classifier
* **Interpretable**: Representation that is understandable to humans
* **Model-agnostic**: Applied to any black box model
* **Explanations**: A statement that makes something clear. 


The purpose of lime is to explain the predictions of black box classifiers. What this means is that for any given prediction and any given classifier it is able to determine a small set of features in the original data that has driven the outcome of the prediction.

## AUTHOR WORDS

In 2016 **Tulio Ribeiro et al.** proposed LIME method for explaining prediction for a single observation which takes a significantly different approach compared to the methods described above. The algorithm is designed to explain predictions of any classifier and it works primarily for image and text data. First, original observation of interest is transformed into simplified input space of binary vectors (for example presence or absence of words). Then a dataset of similar observations is created by sampling features that are present in the representation of the explained instance. Closeness of these observations to the original observations is measured via specified similarity kernel. This distance is taken into account while the explanation model is fitted to the new dataset. The interpretable model can be penalized to assure that it does not become too complex itself.


![](https://image.ibb.co/coAgxe/lime1.png)


Explaining individual predictions. A model predicts that a patient has the flu, and LIME highlights the symptoms in the patient’s history that led to the prediction. Sneeze and headache are portrayed as contributing to the “flu” prediction, while “no fatigue” is evidence against it. With these, a doctor can make an informed decision about whether to trust the model’s prediction.


## EXAMPLE TO PRESENT INTUITION FOR LIME


![](https://image.ibb.co/jUxVOK/lime2.png)


The black-box model’s complex decision function f (unknown to LIME) is represented by the blue/pink background, which cannot be approximated well by a linear model. The bold red cross is the instance being explained. LIME samples instances, gets predictions using f, and weighs them by the proximity to the instance being explained (represented here by size). The dashed line is the learned explanation that is locally (but not globally) faithful.

## ALGORITHM


**1. Permute Data: ** It take observations and create fake data for it. It permuted in different ways.

**2. Calculate distance between permutations and original observations: ** Then it calculate the distance metric and similarity score those fake data and original one.

**3. Make predictions on new data using complex: ** Then it take black box algorithm and make prediction on that dataset.

**4. Pick m features best describing the complex model outcome from the permuted data: ** Then it tries different combinations of predictors ie. m number to figure out minimum number of predictor you have that gives you maximum likelihood of the class that was predicted by the black box.

**5. Fit a simple model to the permuted data with m features and similarity scores as weights: ** Finally it picks those M features and similarity scores and fits the simple model ie. linear model.

**6. Feature weights from the simple model make explanations for the complex models local behaviour: ** The coefficient will serve as an explanation.


## WHY IS IT IMPORTANT 


Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: 
1.  trusting a prediction, i.e. whether a user trusts an individual prediction sufficiently to take some action based on it, and 
2. trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by how much the human understands a model’s behaviour, as opposed to seeing it as a black box.


![](https://image.ibb.co/dSwtSe/lime3.png)


# DEMONSTRATION

## LOADING NECESSARY PACKAGES

```{r}
#load packages
library(readr)
library(lime)
library(xgboost)
library(caret)
library(dplyr)
library(tibble)
library(text2vec) 
library(qdapRegex)
```

## LOADING TWEETS DATA

```{r}
tweets <- read_csv('../input/Tweets.csv')
head(tweets,2)
```

## DATA CLEANING

let’s clean the data a little: select only tweets text and sentiment, change column names to something more readable and remove URLs from text.

```{r}

all_tweets <- tweets %>% 
  rename(sentiment = airline_sentiment) %>% 
  select(sentiment, text) %>% 
  mutate(text = qdapRegex::rm_url(text)) %>% #removes URLs from text
  na.omit()
  
head(all_tweets)

```

## SPLIT DATA INTO TRAIN & TEST

it’s time to split it into train and test sets - caret package

```{r}

set.seed(3107)
trainIndex <- createDataPartition(all_tweets$sentiment, p = .8, 
                                  list = FALSE, 
                                  times = 1)

train_tweets <- all_tweets[ trainIndex,]
test_tweets <- all_tweets[ -trainIndex,]

train_tweets$sentiment <- as.numeric(as.factor(train_tweets$sentiment)) -1
test_tweets$sentiment <- as.numeric(as.factor(test_tweets$sentiment)) -1

str(train_tweets)

```
## TRANSFORM INTO DOCUMENT TERM MATRICES

A document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.

```{r}
# tokenizes text data nad creates Document Term Matrix
get_matrix <- function(text) {
  it <- itoken(text, progressbar = FALSE)
  create_dtm(it, vectorizer = hash_vectorizer())
}

dtm_train= get_matrix(train_tweets$text)
dtm_test = get_matrix(test_tweets$text)
```

## MAKING MODEL: XGBOOST

Now, time for the model. I used Extreme Gradient Boosting tree model for classification. I made only two type of sentiment i.e. negative & positive by combining neutral with positive sentiment and used 'binary:logistic' objective function. 

```{r}

param <- list(max_depth = 7, 
              eta = 0.1, 
              objective = "binary:logistic", 
              eval_metric = "error", 
              nthread = 1)

set.seed(3107)
xgb_model <- xgb.train(
  param, 
  xgb.DMatrix(dtm_train, label = train_tweets$sentiment!=0),
  nrounds = 50,
  verbose=0
)

# We use a (standard) threshold of 0.5
predictions <- predict(xgb_model, dtm_test) > 0.5
test_labels <- test_tweets$sentiment!=0

# Accuracy
print(mean(predictions == test_labels))

```

Approx 79% accuracy was quite impressive, given how little we did in terms of data pre-processing and feature engineering. 

## ARE ABOVE MODEL: XGBOOST REASONABLE

In order to understand this, I’ll run lime’s **explainer()** only on correctly predicted instances while ignoring misclassified observations. I’ll pick first 5 observations for interpretation.


```{r}

# select only correct predictions
predictions_tb = predictions %>% as_tibble() %>% 
  rename_(predict_label = names(.)[1]) %>%
  tibble::rownames_to_column()

correct_pred = test_tweets %>%
  tibble::rownames_to_column() %>% 
  mutate(test_label = sentiment != 0) %>%
  left_join(predictions_tb) %>%
  filter(test_label == predict_label) %>% 
  pull(text) %>% 
  head(5)

```

Let’s just define the explainer and run it on the sample of correctly predicted tweets.

```{r}

explainer <- lime(correct_pred, model = xgb_model, 
                  preprocess = get_matrix)

corr_explanation <- lime::explain(correct_pred, explainer, n_labels = 1, 
                       n_features = 6, cols = 2, verbose = 0)

```

Ready? Here we go! 

LIME **plot_features** function used to plot features which explains the prediction.

```{r}
plot_features(corr_explanation)
```

**Label == 1  is positive sentiment and Label == 0  is negative sentiment ** 

> Actually model looks good by accuracy but internally it is not so strong. Yes, For positive sentiment ***love and great*** word make some sence and For negative sentiment ***no* **make some sence. 

If you feel somehow ill at ease with this kind of presentation of model interpretations, try running **plot_text_explanations()** instead:

```{r}
plot_text_explanations(corr_explanation)
```

# CONCLUSION

Trust is crucial for effective human interaction with machine learning systems, and that explaining individual predictions is important in assessing trust. Explanations are useful for a variety of models in trust-related tasks in the text and image domains, with both expert and non-expert users: deciding between models, assessing trust, improving untrustworthy models, and getting insights into predictions

# REFERENCES

Offical Paper: "Why Should I Trust You?": Explaining the Predictions of Any Classifier
https://arxiv.org/abs/1602.04938

LIME Package: Local Interpretable Model-Agnostic Explanations (R port of original Python package)
https://github.com/thomasp85/lime

Kdnuggets Blog: Introduction to Local Interpretable Model-Agnostic Explanations (LIME)

PPT by Kasia Kulma (PhD), Data Scientist, Aviva: Interpretable Machine Learning Using LIME Framework
https://www.slideshare.net/0xdata/interpretable-machine-learning-using-lime-framework-kasia-kulma-phd-data-scientist

LIME Package Python: Lime: Explaining the predictions of any machine learning classifier
https://github.com/marcotcr/lime