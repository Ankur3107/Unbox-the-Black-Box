---
title: "Unbox the Block Box: (Part #1)"
author: "Ankur Singh"
date: "Aug 23, 2018"
output:
 html_document:
    fig_width: 10
    fig_height: 7
    toc: yes
    number_sections : yes
    code_folding: show
---

<center><img src="https://www.tibco.com/blog/wp-content/uploads/2018/06/BeyondBlackBox_image620x360.jpg"></center>


<hr>

<strong>Version Description :</strong>

* _version 1 : initial commit_ 

<hr>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE)
```

# OBJECTIVE

Actually I was thinking about 'how to get the machine learning model explantion'. Then I searched on Google and found a great package called **LIME** by Tulio Ribeiro, Sameer Singh and Carlos Guestrin. I  was very impressed with the package. In this kernel, I will share my knowledge that i got from different papers, blogs and videos.


# LIME : Locally Interpretable Model-agnostic Explanations

<center><iframe width="560" height="315" src="https://www.youtube.com/embed/hUnRCxnydCc" frameborder="0" allowfullscreen></iframe></center>

## ABSTRACT

Machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.

## WHAT IS LIME

* **Locally**: Locally faithful to the classifier
* **Interpretable**: Representation that is understandable to humans
* **Model-agnostic**: Applied to any black box model
* **Explanations**: A statement that makes something clear. 


The purpose of lime is to explain the predictions of black box classifiers. What this means is that for any given prediction and any given classifier it is able to determine a small set of features in the original data that has driven the outcome of the prediction.

## AUTHOR WORDS

In 2016 **Tulio Ribeiro et al.** proposed LIME method for explaining prediction for a single observation which takes a significantly different approach compared to the methods described above. The algorithm is designed to explain predictions of any classifier and it works primarily for image and text data. First, original observation of interest is transformed into simplified input space of binary vectors (for example presence or absence of words). Then a dataset of similar observations is created by sampling features that are present in the representation of the explained instance. Closeness of these observations to the original observations is measured via specified similarity kernel. This distance is taken into account while the explanation model is fitted to the new dataset. The interpretable model can be penalized to assure that it does not become too complex itself.


![](https://image.ibb.co/coAgxe/lime1.png)


Explaining individual predictions. A model predicts that a patient has the flu, and LIME highlights the symptoms in the patient’s history that led to the prediction. Sneeze and headache are portrayed as contributing to the “flu” prediction, while “no fatigue” is evidence against it. With these, a doctor can make an informed decision about whether to trust the model’s prediction.


## EXAMPLE TO PRESENT INTUITION FOR LIME


![](https://image.ibb.co/jUxVOK/lime2.png)


The black-box model’s complex decision function f (unknown to LIME) is represented by the blue/pink background, which cannot be approximated well by a linear model. The bold red cross is the instance being explained. LIME samples instances, gets predictions using f, and weighs them by the proximity to the instance being explained (represented here by size). The dashed line is the learned explanation that is locally (but not globally) faithful.

## ALGORITHM


**1. Permute Data: ** It take observations and create fake data for it. It permuted in different ways.

**2. Calculate distance between permutations and original observations: ** Then it calculate the distance metric and similarity score those fake data and original one.

**3. Make predictions on new data using complex: ** Then it take black box algorithm and make prediction on that dataset.

**4. Pick m features best describing the complex model outcome from the permuted data: ** Then it tries different combinations of predictors ie. m number to figure out minimum number of predictor you have that gives you maximum likelihood of the class that was predicted by the black box.

**5. Fit a simple model to the permuted data with m features and similarity scores as weights: ** Finally it picks those M features and similarity scores and fits the simple model ie. linear model.

**6. Feature weights from the simple model make explanations for the complex models local behaviour: ** The coefficient will serve as an explanation.


## WHY IS IT IMPORTANT 


Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: 
1.  trusting a prediction, i.e. whether a user trusts an individual prediction sufficiently to take some action based on it, and 
2. trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by how much the human understands a model’s behaviour, as opposed to seeing it as a black box.


![](https://image.ibb.co/dSwtSe/lime3.png)


# DEMONSTRATION

## LOADING NECESSARY PACKAGES

```{r}
#load packages
library(readr)
library(lime)
library(xgboost)
library(caret)
library(dplyr)
library(tibble)
library(text2vec) 
library(qdapRegex)
```

## LOADING TWEETS DATA

```{r}
tweets <- read_csv('../input/Tweets.csv')
head(tweets,2)
```

## DATA CLEANING

let’s clean the data a little: select only tweets text and sentiment, change column names to something more readable and remove URLs from text.

```{r}

all_tweets <- tweets %>% 
  rename(sentiment = airline_sentiment) %>% 
  select(sentiment, text) %>% 
  mutate(text = qdapRegex::rm_url(text)) %>% #removes URLs from text
  na.omit()
  
head(all_tweets)

```

## SPLIT DATA INTO TRAIN & TEST

it’s time to split it into train and test sets - caret package

```{r}

set.seed(3107)
trainIndex <- createDataPartition(all_tweets$sentiment, p = .8, 
                                  list = FALSE, 
                                  times = 1)

train_tweets <- all_tweets[ trainIndex,]
test_tweets <- all_tweets[ -trainIndex,]

train_tweets$sentiment <- as.numeric(as.factor(train_tweets$sentiment)) -1
test_tweets$sentiment <- as.numeric(as.factor(test_tweets$sentiment)) -1

str(train_tweets)

```
## TRANSFORM INTO DOCUMENT TERM MATRICES

A document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.

```{r}
# tokenizes text data nad creates Document Term Matrix
get_matrix <- function(text) {
  it <- itoken(text, progressbar = FALSE)
  create_dtm(it, vectorizer = hash_vectorizer())
}

dtm_train= get_matrix(train_tweets$text)
dtm_test = get_matrix(test_tweets$text)
```

